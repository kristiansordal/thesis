\chapter{Related Work}
SpMV has been a central topic of research in high performance computing due to its widespread use in iterative solvers and numerical simulations. Over the past decades, various approaches have been proposed to optimize SpMV on both shared and distributed memory systems, focusing on reducing communication overhead, improving memory access locality, and achieving effective load balancing. The inherent sparsity and irregularity of memory access patterns in SpMV make it particularly challenging to parallelize efficiently, especially at scale. Consequently, significant efforts have been devoted to designing storage formats, partitioning strategies, and communication models that better leverage modern hardware architectures. This section provides an overview of key contributions in this area, with an emphasis on partitioning techniques, memory and communication models, and recent innovations in hybrid parallelism.
\medskip

Merrill and Garland \cite{merrilduane} propose a merge-based parallel SpMV algorithm that operates directly on the Compressed Sparse Row (CSR) format without requiring reformatting or auxiliary data structures. Their method employs a 2D merge-path decomposition that ensures strictly balanced workloads across processing elements, independent of row length variability. This design addresses the primary bottleneck of irregular row distributions in parallel SpMV, which often lead to performance inconsistencies in traditional row- and nonzero-splitting strategies. Evaluated across over 4,000 matrices, their approach demonstrated superior performance consistency and scalability, particularly on architectures with constrained local memories such as NUMA and GPUs. This work offers a compelling alternative to format-specific optimizations by achieving portability and high throughput directly on CSR representations.
\medskip

Trotter et al. \cite{ordersparse} present a comprehensive evaluation of matrix reordering strategies for enhancing sparse matrix-vector multiplication (SpMV) performance on multicore CPUs. Evaluating six reordering algorithms over 490 matrices on eight architectures, they find that reorderings based on graph and hypergraph partitioning (e.g., METIS, PaToH) offer the most consistent SpMV performance benefits. Their results indicate that performance improvements arise primarily from better data locality and reduced off-diagonal nonzero density, while bandwidth and profile reductions are less influential. They also underscore the architectural sensitivity of reordering effectiveness, suggesting that careful algorithm-architecture matching is critical. This study provides empirical validation for the effectiveness of reordering as a pre-processing optimization for SpMV, especially in shared-memory systems.



% add work about GPU CSR SPMV.


