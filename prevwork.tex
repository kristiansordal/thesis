\chapter{Related Work}
Merrill and Garland \cite{merrilduane} propose a merge-based parallel SpMV algorithm that operates directly on the Compressed Sparse Row (CSR) format without requiring reformatting or auxiliary data structures. Their method employs a 2D merge-path decomposition that ensures strictly balanced workloads across processing elements, independent of row length variability. This design addresses the primary bottleneck of irregular row distributions in parallel SpMV, which often lead to performance inconsistencies in traditional row- and nonzero-splitting strategies. Evaluated across over 4,000 matrices, their approach demonstrated superior performance consistency and scalability, particularly on architectures with constrained local memories such as NUMA and GPUs. This work offers a compelling alternative to format-specific optimizations by achieving portability and high throughput directly on CSR representations.
\medskip

Trotter et al. \cite{ordersparse} present a comprehensive evaluation of matrix reordering strategies for enhancing sparse matrix-vector multiplication (SpMV) performance on multicore CPUs. Evaluating six reordering algorithms over 490 matrices on eight architectures, they find that reorderings based on graph and hypergraph partitioning (e.g., METIS, PaToH) offer the most consistent SpMV performance benefits. Their results indicate that performance improvements arise primarily from better data locality and reduced off-diagonal nonzero density, while bandwidth and profile reductions are less influential. They also underscore the architectural sensitivity of reordering effectiveness, suggesting that careful algorithm-architecture matching is critical. This study provides empirical validation for the effectiveness of reordering as a pre-processing optimization for SpMV, especially in shared-memory systems.

