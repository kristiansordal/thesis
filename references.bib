@inproceedings{Rosen12lrec,
    Author = {Ros{\'e}n, Victoria and De Smedt, Koenraad and Meurer, Paul and
              Dyvik, Helge},
    Booktitle = {META-RESEARCH Workshop on Advanced Treebanking at LREC2012},
    Date-Added = {2014-11-12 14:36:36 +0000},
    Date-Modified = {2014-11-12 14:36:36 +0000},
    Editor = {Haji\v{c}, Jan and De Smedt, Koenraad and Tadi\'c, Marko and
              Branco, Ant{\'o}nio},
    Pages = {22--29},
    Title = {An Open Infrastructure for Advanced Treebanking},
    Year = {2012},
}
	Bdsk-Url-1 = {http://www.lrec-conf.org/proceedings/lrec2012/workshops/12.LREC%202012%20Advanced%20Treebanking%20Proceedings.pdf}}
}
@inproceedings{gupta2024gamgi,
    author = {Gautam Gupta and Sivasankaran Rajamanickam and Erik G. Boman},
    title = {{GAMGI}: Communication-Reducing Algebraic Multigrid for GPUs},
    booktitle = {Proceedings of the 28th ACM SIGPLAN Annual Symposium on
                 Principles and Practice of Parallel Programming (PPoPP)},
    year = {2024},
    pages = {61--75},
    publisher = {ACM},
    doi = {10.1145/3673038.3673042},
}

@book{Van-Dongen12,
    Author = {Van Dongen, Marc},
    Date-Added = {2014-11-11 18:56:17 +0000},
    Date-Modified = {2014-11-11 18:56:17 +0000},
    Isbn/Issn = {978-3-642-23816-1},
    Pid = {10.1007/978-3-642-23816-1},
    Publisher = {Springer},
    Title = {{LaTeX} and Friends},
    Url = {http://csweb.ucc.ie/~dongen/LAF/LAF.html},
    Year = {2012},
}

@book{Gries09,
    Author = {Gries, Stefan},
    Booktitle = {Quantitative Corpus Linguistics with {R}: A Practical
                 Introduction},
    Date-Added = {2012-12-06 12:12:26 +0000},
    Date-Modified = {2012-12-06 12:12:26 +0000},
    Isbn/Issn = {978-0415962704},
    Publisher = {Routledge},
    Title = {Quantitative Corpus Linguistics with {R}: A Practical Introduction},
    Year = {2009},
}

@misc{norvig_latency,
    author = {Peter Norvig},
    title = {Latency Numbers Every Programmer Should Know},
    year = {2021},
    howpublished = {\url{https://norvig.com/21-days.html#Latency}},
    note = {Accessed: 2025-04-29},
}


@article{ellpackformat,
    title = {BiELL: A bisection ELLPACK-based storage format for optimizing SpMV
             on GPUs},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {74},
    number = {7},
    pages = {2639-2647},
    year = {2014},
    note = {Special Issue on Perspectives on Parallel and Distributed Processing
            },
    issn = {0743-7315},
    doi = {https://doi.org/10.1016/j.jpdc.2014.03.002},
    url = {https://www.sciencedirect.com/science/article/pii/S0743731514000458},
    author = {Cong Zheng and Shuo Gu and Tong-Xiang Gu and Bing Yang and
              Xing-Ping Liu},
    keywords = {Sparse matrix–vector multiplication, GPU, BiELL, BiJAD},
    abstract = {Sparse matrix–vector multiplication (SpMV) is one of the most
                important high level operations for basic linear algebra.
                Nowadays, the GPU has evolved into a highly parallel coprocessor
                which is suited to compute-intensive, highly parallel
                computation. Achieving high performance of SpMV on GPUs is
                relatively challenging, especially when the matrix has no
                specific structure. For these general sparse matrices, a new data
                structure based on the bisection ELLPACK format, BiELL, is
                designed to realize the load balance better, and thus improve the
                performance of the SpMV. Besides, based on the same idea of JAD
                format, the BiJAD format can be obtained. Experimental results on
                various matrices show that the BiELL and BiJAD formats perform
                better than other similar formats, especially when the number of
                non-zero elements per row varies a lot.},
}
@article{manchanda2010non,
    title = {Non-uniform memory access (numa)},
    author = {Manchanda, Nakul and Anand, Karan},
    journal = {New York University},
    volume = {4},
    year = {2010},
}
@misc{llnlparallel,
    author = {{Lawrence Livermore National Laboratory}},
    title = {Introduction to Parallel Computing Tutorial},
    year = {2024},
    howpublished = {\url{
                    http://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial
                    }},
    note = {Accessed: 2025-05-08},
}
@techreport{karypis1997metis,
    author = {George Karypis and Vipin Kumar},
    title = {{METIS: A Software Package for Partitioning Unstructured Graphs,
             Partitioning Meshes, and Computing Fill-Reducing Orderings of Sparse
             Matrices}},
    year = {1997},
    institution = {University of Minnesota, Department of Computer Science /
                   Army HPC Research Center},
    note = {Version 3.0.3},
    url = {https://hdl.handle.net/11299/215346},
}
@article{hypergraphpartitioning,
    author = {Catalyurek, U.V. and Aykanat, C.},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    title = {Hypergraph-partitioning-based decomposition for parallel
             sparse-matrix vector multiplication},
    year = {1999},
    volume = {10},
    number = {7},
    pages = {673-693},
    keywords = {Vectors;Matrix decomposition;Sparse matrices;Concurrent
                computing;Computational modeling;Testing;Communication
                standards;Parallel processing;Linear systems;Equations},
    doi = {10.1109/71.780863},
}

@article{10.3389/fphy.2023.979699,
    AUTHOR = {Burchard, Luk and Hustad, Kristian Gregorius and Langguth,
              Johannes and Cai, Xing },
    TITLE = {Enabling unstructured-mesh computation on massively tiled AI
             processors: An example of accelerating in silico cardiac simulation},
    JOURNAL = {Frontiers in Physics},
    VOLUME = {Volume 11 - 2023},
    YEAR = {2023},
    URL = {
           https://www.frontiersin.org/journals/physics/articles/10.3389/fphy.2023.979699
           },
    DOI = {10.3389/fphy.2023.979699},
    ISSN = {2296-424X},
    ABSTRACT = {<p>A new trend in processor architecture design is the packaging
                of thousands of small processor cores into a single device, where
                there is no device-level shared memory but each core has its own
                local memory. Thus, both the work and data of an application code
                need to be carefully distributed among the small cores, also
                termed as <italic>tiles</italic>. In this paper, we investigate
                how numerical computations that involve unstructured meshes can
                be efficiently parallelized and executed on a massively tiled
                architecture. Graphcore IPUs are chosen as the target hardware
                platform, to which we port an existing monodomain solver that
                simulates cardiac electrophysiology over realistic 3D irregular
                heart geometries. There are two computational kernels in this
                simulator, where a 3D diffusion equation is discretized over an
                unstructured mesh and numerically approximated by repeatedly
                executing sparse matrix-vector multiplications (SpMVs), whereas
                an individual system of ordinary differential equations (ODEs) is
                explicitly integrated per mesh cell. We demonstrate how a new
                style of programming that uses Poplar/C++ can be used to port
                these commonly encountered computational tasks to Graphcore IPUs.
                In particular, we describe a per-tile data structure that is
                adapted to facilitate the inter-tile data exchange needed for
                parallelizing the SpMVs. We also study the achievable performance
                of the ODE solver that heavily depends on special mathematical
                functions, as well as their accuracy on Graphcore IPUs. Moreover,
                topics related to using multiple IPUs and performance analysis
                are addressed. In addition to demonstrating an impressive level
                of performance that can be achieved by IPUs for monodomain
                simulation, we also provide a discussion on the generic theme of
                parallelizing and executing unstructured-mesh multiphysics
                computations on massively tiled hardware.</p>},
}

@misc{cerebras2024wse3,
    author = {Cerebras Systems Inc.},
    title = {WSE-3 Datasheet},
    year = {2024},
    howpublished = {\url{https://cerebras.net/product/system/}},
    note = {DS03 v3 821},
    institution = {Cerebras Systems Inc.},
    address = {1237 E. Arques Ave, Sunnyvale, CA 94085, USA},
}

@inproceedings{merrilduane,
    author = {Merrill, Duane and Garland, Michael},
    booktitle = {SC '16: Proceedings of the International Conference for High
                 Performance Computing, Networking, Storage and Analysis},
    title = {Merge-Based Parallel Sparse Matrix-Vector Multiplication},
    year = {2016},
    volume = {},
    number = {},
    pages = {678-689},
    keywords = {Sparse matrices;Graphics processing
                units;Runtime;Encoding;Matrix decomposition;Parallel
                processing;Partitioning algorithms;SpMV;sparse matrix;parallel
                merge;merge-path;many-core;GPU;linear algebra},
    doi = {10.1109/SC.2016.57},
}
@inproceedings{ordersparse,
    author = {Trotter, James D. and Ekmek\c{c}iba\c{s}\i{}, Sinan and Langguth,
              Johannes and Torun, Tugba and D\"{u}zak\i{}n, Emre and Ilic,
              Aleksandar and Unat, Didem},
    title = {Bringing Order to Sparsity: A Sparse Matrix Reordering Study on
             Multicore CPUs},
    year = {2023},
    isbn = {9798400701092},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3581784.3607046},
    doi = {10.1145/3581784.3607046},
    abstract = {Many real-world computations involve sparse data structures in
                the form of sparse matrices. A common strategy for optimizing
                sparse matrix operations is to reorder a matrix to improve data
                locality. However, it's not always clear whether reordering will
                provide benefits over the unordered matrix, as its effectiveness
                depends on several factors, such as structural features of the
                matrix, the reordering algorithm and the hardware that is used.
                This paper aims to establish the relationship between matrix
                reordering algorithms and the performance of sparse matrix
                operations. We thoroughly evaluate six different matrix
                reordering algorithms on 490 matrices across eight multicore
                architectures, focusing on the commonly used sparse matrix-vector
                multiplication (SpMV) kernel. We find that reordering based on
                graph partitioning provides better SpMV performance than the
                alternatives for a large majority of matrices, and that the
                resulting performance is explained through a combination of data
                locality and load balancing concerns.},
    booktitle = {Proceedings of the International Conference for High
                 Performance Computing, Networking, Storage and Analysis},
    articleno = {31},
    numpages = {13},
    keywords = {matrix reordering, sparse matrix-vector multiply, multicore,
                graph partitioning},
    location = {Denver, CO, USA},
    series = {SC '23},
}

@article{10064025,
    author = {Thune, Andreas and Reinemo, Sven-Arne and Skeie, Tor and Cai, Xing
              },
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    title = {Detailed Modeling of Heterogeneous and Contention-Constrained
             Point-to-Point MPI Communication},
    year = {2023},
    volume = {34},
    number = {5},
    pages = {1580-1593},
    keywords = {Bandwidth;Sockets;Benchmark testing;Size
                measurement;Protocols;Multicore processing;Computational
                modeling;Intra-node communication;performance
                modeling;point-to-point MPI communication},
    doi = {10.1109/TPDS.2023.3253881},
}
