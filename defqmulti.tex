\subsubsection{One MPI rank per node}
\begin{figure}[H]
    \centering
    \incfig[1]{gflops\_2x4\_multi\_defq}
    \caption{Achieved GFLOPS on multi node setup using the \defq{} chip.}
    \label{fig:gflopsdefqmulti}
\end{figure}
% The results presented in Figure \ref{fig:gflopsdefqmulti} illustrate the GFLOPS achieved when running SpMV for 100 iterations in a distributed and shared memory hybrid environment. For these results, 4 nodes running dual-socketed \defq{} chips have been used, with one MPI rank placed on each node. The results largely align with what is expected, as each consecutive communication strategy outperforms the previous.
\medskip

The results illustrate some quirks that are inherent to the \defq{} chip. As is evident from the plots, using only one node (and one MPI rank), consistently yield results that are either better or almost as good as the results achieved when using the maximum amount of available nodes, and when compared to using 64 MPI ranks - one per physical core - the performance is much worse. As an example, for the \texttt{Bump\_2911} matrix, strategy D achieves a GFLOPS performance of \(\approx 9\) GFLOPS on one node, with 64 OpenMP shared memory threads. When this is compared to Figure \ref{fig:gflopsdefqsingle} using 64 MPI ranks, which achieves \(\approx 30\) GFLOPS for the same matrix.
\medskip

It is clear that using only distributed memory far outperforms using only shared memory parallelization. The reasons for this is outside the scope of this thesis, but in brief, the \defq{} utilizes the first generation Zen microarchitecture, which had some problems in this area. (ADD REFERENCE HERE).

% chip is a 1st generation chip, and does have a weak core to core connection, which could be the reason. 
% \medskip

\begin{figure}[H]
    \centering
    \incfig[1]{tcomm\_2x4\_multi\_defq}
    \caption{}
    \label{fig:tcommdefqmulti}
\end{figure}


\begin{figure}[H]
    \centering
    \incfig{commload\_2x4\_multi\_defq}
    \caption{}
    \label{}
\end{figure}

\subsubsection{One MPI rank per socket}

\begin{figure}[H]
    \centering
    \incfig[1]{gflops\_2x4\_multi\_defqmpi}
    \caption{Multi Node - \defq}
    \label{fig:gflopsdefqmpimulti}
\end{figure}


Figure \ref{fig:gflopsdefqmpimulti} illustrate the achieved GFLOPS when placing one MPI rank per socket. The benefits of doing this is that we get to utilize the memory controllers on both chips, instead of only one when placing one MPI rank per node.
\medskip

When compared to Figure \ref{fig:gflopsdefqmulti}, we see that the performance drops when using only one node has drastically dropped, but that most communication strategies performs better with this configuration when scaling to multiple nodes. However, even when using four nodes, the performance is not as good as using a single node with one MPI rank.

\begin{figure}[H]
    \centering
    \incfig[1]{tcomm\_2x4\_multi\_defqmpi}
    \caption{}
    \label{fig:tcommdefqmpimulti}
\end{figure}


\begin{figure}[H]
    \centering
    \incfig{commload\_2x4\_multi\_defqmpi}
    \caption{}
    \label{}
\end{figure}
