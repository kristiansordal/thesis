\chapter{Results} \label{results}

\section{Theoretical Maximum}
As has been established, the maximum performance scales with the available memory bandwidth of the system, and not with the amount of processes used. For SpMV, a total of 6 bytes are read per FLOP, which means that the theoretical maximum is given by \ref{eq:theoretical_max}. This is under the assumption that all resources on the system is dedicated to the SpMV operation, which is not the case in reality. At most, somewhere in the neighbourhood of 80\% of the systems resources can be expected to be utilized.

\begin{equation}
\text{Maximum Theoretical Performance} = \frac{\text{Memory bandwidth}}{6}
\label{eq:theoretical_max}
\end{equation}


\section{Experimental Setup}
In order to rigorously evaluate and compare the performance of the five communication strategies, a systematic series of experiments were ran on the eX\(^{3}\) machine. Each experiment executes 100 iterations of SpMV on a given sparse matrix, with a given configuration and a given communication strategy. Every program used has been compiled with the compiler flags \texttt{-march=native} and \texttt{-O3}.
 
\subsection{Hardware Platforms}

The experiments that have been ran target three high‚Äêperformance computing systems. \autoref{tab:hardware} gives an overview of the hardware that has been used in the experiments.

\input{cputable}


There has been performed experiments measuring both single node and multi node performance. For single node experiments, the number of MPI ranks is doubled until all cores on the given node is assigned a MPI rank, also OpenMP was not utilized for these experiments.
\medskip

For multi node experiments, two configuration have been tested. The first configuration involves assinging one MPI rank per node, and utilizing shared memory paralleization through the use of OpenMP within each node. The second configuration places one MPI rank per socket. These experiments aims to show the performance of the communication strategies on multi node systems. Furthermore, it is possible to compare the impact that the number of memory controllers utilized have on the results. Using one MPI rank per node only utilizes one of two memory controllers on a dual socketed systems, whereas using one MPI rank per socket utilizes both of the memory controller on the node.



\subsection{Communication Strategies}
The communication strategies used have been extensively discussed in \ref{sec:commstrats}, and \autoref{tab:commstratdesc} gives an overview of what the various communication strategies will be referred to in the folliwing sections, and the key idea they use to communicate the data-dependencies between ranks.

\begin{table}[H]
    \caption{Names and description of communication strategies that have been tested.}
    \label{tab:commstratdesc}
    \begin{center}
        \begin{tabular}[c]{|p{3cm}|p{8.5cm}|}
            \hline
             \textbf{Strategy name}& \textbf{Strategy Description}  \\
            \hline
             Strategy A&Exchanges entire local vector.  \\
            \hline
             Strategy B&Exchanges entire separator.  \\
            \hline
             Strategy C&Exchanges required separator.  \\
            \hline
             Strategy D&Exchanges required separator elements.  \\
            \hline
             Strategy E&Exchanges required separator elements, memory scalable.  \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

\subsection{Matrices}
As the field of interest is that of results from scientific computing, the matrices shown in Figure \ref{fig:matricesused} have been selected, and \autoref{tab:matricesused} gives a brief description of the problem the matrices solve. Matrices that are based on scientific computing problems are generally well structured and have nonzero entries centered around the main diagonal. Matrices with these kinds of structures tend to yield a larger cache hit rate than matrices based on social networks and other kinds of graphs. It is important to keep this in mind when interpreting the results, as other kinds of graphs can yield very different results.

\begin{figure}[H]
  \centering
  \captionsetup[sub]{font=tiny, textfont=tiny}
  \setlength{\tabcolsep}{3pt} % adjust horizontal padding between columns
  \begin{tabular}{cccc}
      \subcaptionbox{\tiny{bone010}\label{fig:bone010}}{%
      \includegraphics[width=0.23\textwidth]{bone010.png}%
    } &
    \subcaptionbox{\tiny{af\_shell10}\label{fig:af_shell10}}{%
      \includegraphics[width=0.23\textwidth]{af_shell10.png}%
    } &
    \subcaptionbox{\tiny{Serena}\label{fig:Serena}}{%
      \includegraphics[width=0.23\textwidth]{Serena.png}%
    } &
    \subcaptionbox{\tiny{Long\_Coup\_dt0}\label{fig:Long_Coup_dt0}}{%
      \includegraphics[width=0.23\textwidth]{Long_Coup_dt0.png}%
    } \\[6pt] % a little extra vertical gap
    \subcaptionbox{\tiny{dielFilterV3real}\label{fig:dielFilterV3real}}{%
      \includegraphics[width=0.23\textwidth]{dielFilterV3real.png}%
    } &
    \subcaptionbox{\tiny{Cube\_Coup\_dt0}\label{fig:Cube_Coup_dt0_1}}{%
      \includegraphics[width=0.23\textwidth]{Cube_Coup_dt0.png}%
    } &
    \subcaptionbox{\tiny{Bump\_2911}\label{fig:Bump_2911}}{%
      \includegraphics[width=0.23\textwidth]{Bump_2911.png}%
    } &
    \subcaptionbox{\tiny{nlpkkt200}\label{fig:nlpkkt200}}{%
      \includegraphics[width=0.23\textwidth]{nlpkkt200.png}%
    }
  \end{tabular}
  \caption{Matrices used to generate results.}
  \label{fig:matricesused}
\end{figure}

\begin{table}[H]
\begin{center}
\begin{tabular}[c]{|p{3cm}|p{8.5cm}|}
\hline
\textbf{Name}&\textbf{Purpose}  \\
\hline
bone010&Trabecular Bone Micro-Finite Element Model\\
\hline
af\_shell1&Sheet metal forming matrix\\
\hline
Serena&Gas reservoir simulation for \(CO_{2}\) sequestration\\
\hline
Long\_Coup\_dt0&3D coupled consolidation problem (geological formation)\\
\hline
dielFilterV3real&High-order vector finite element method in EM\\
\hline
Cube\_Coup\_dt0&3D coupled consolidation problem (3D cube)\\
\hline
Bump\_2911&3D geomechanical reservoir simulation\\
\hline
nlpkkt200&Symmetric indefinite KKT matrix\\
\hline
\end{tabular}
\end{center}
\caption{Names and descriptions of the matrices used in the experiments.}
\label{tab:matricesused}
\end{table}

\section{\defq{}}
The results in the following section present the performance metrics of 100 iteration of SpMV on various configurations using the \defq{} chip. For this hardware, both single node and multi node experiments have been performed.
\medskip


For the multi node experiments, 

\subsection{Single Node Performance}
\label{sec:singlenode}
For the single node experiments, no shared memory paralleization has been utilized. Each communication strategy has been ran using \(1,2,4,8,16,32\) and \(64\) MPI ranks, as the dual socketed system provides two 32-core \defq{} chips.
\medskip

\input{defqsingle}

\subsection{Multi Node Performance}
The results in this section present the performance metrics when ran on one through four dual-socketed nodes with two \defq{} chips on each node. Two sets of results are presented, the first uses a configuration where only one MPI rank has been placed on each node, and the second places two MPI ranks per node, or in other words, one MPI rank per socket. On both configurations, 64 OpenMP threads have been used, as each chip have 32 physical cores. These results aim to illustrate the difference in performance in a multi-node configuration, both when utilizing only one memory controller (i.e. one MPI rank per node), and when utilizing two memory controllers (i.e. one MPI rank per socket). 
\medskip
\input{defqmulti}


\section{\romeq{}}
\input{romemulti.tex}

\section{\fpgaq{}}
\input{fpgaqmulti.tex}
