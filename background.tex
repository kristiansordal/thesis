\chapter{Background}

\section{Sparse Matrix-Vector Multiplication}

Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation encountered in many areas of scientific computing. It is especially prominent in solving large systems of linear equations and in large-scale simulations. The matrices involved are typically both very large and very sparse. 

A matrix can in theory be considered sparse if it is worthwhile to treat zero values separately. In theory, this translates to a matrix being less than full, i.e. less than \( \mathcal{O}\left(n^2\right)\) nonzeros for a \(n \times  n\) matrix. However, in the context of sparse linear algebra, sparse means that there is a constant number of nonzeros per row, i.e. \(\mathcal{O}\left(n\right)\) nonzeros per row. The matrices used in scientific computing, such as matrices based on meshes, or graphs such as social networks all have this property. Optimizing the performance of SpMV, particularly through parallel computing techiniques, is crucial for enhancing the efficiency of many scientific applications.
% In theory, 

% In this context, \textit{sparse} means that the average number of nonzeros per row is on the order of \(\mathcal{O}(1)\), making it worthwhile to store and operate only on nonzero elements explicitly. Optimizing the performance of SpMV, particularly through parallel computing techniques, is crucial for enhancing the efficiency of many scientific applications.

However, SpMV is notoriously difficult to optimize, both in sequential and parallel implementations. One major reason is its inherently low computational density.

\subsection{Computational Density}

The \textit{computational density} of an operation describes the relation between the number of floating-point operations (FLOPS) and the number of memory accesses required. It is formally defined as:

\begin{equation}
    \text{Computational density} = \frac{\text{FLOPS}}{\text{Memory accesses}}
    \label{eq:computationaldensity}
\end{equation}

For SpMV, we read 12 bytes  

Operations with low computational density, such as SpMV, are often \textit{memory bound} rather than \textit{compute bound}. This means that increasing the computational power of a system (e.g., faster processors) does not necessarily lead to proportional speedups in SpMV performance, as memory bandwidth remains the limiting factor.


% (Optional small paragraph to transition)
% As a result, optimizing SpMV performance requires careful consideration of data access patterns, communication overheads, and parallelization strategies. 


\section{CSR Storage Format}
CSR (Compressed Sparse Row) is the most widely used storage format for sparse matrices. As its name suggests, it compresses the amount of memory used to store a matrix without loss of information. It does so by utilizing three vectors \(A_{p}, A_{j}, A_{x}\). Figure \ref{fig:csrformat} shows an example of a matrix stored in CSR format, adapted from \cite{gupta2024gamgi}.



\begin{figure}[ht]
    \centering
    \incfig{csrformat}
    \caption{Example matrix represented in CSR Format.}
    \label{fig:csrformat}
\end{figure}


The first vector, \(A_{p}\) stores the indices of the first nonzero in the vectors \(A_{p}\) and \(A_{x}\). For a given entry \(A_{p}[i]\), \(A_{p}[i]\) is the index of the first nonzero in the \(i^{\text{th}}\) row. \(A_{j}[j]\) and \(A_{x}[j]\) denotes the column index and value of the \(j^{\text{th}}\) nonzero, respectively.
\medskip

A sequential implementation of SpMV on a matrix stored in the CSR format can be implemented in the following manner:
\medskip

\begin{algorithm}[H]
    \caption{Sequential CSR-based SpMV}
    \SetAlgoVlined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{\(A_{p},A_{j},A_{x},x\)}
    \Output{\(y\)\newline}

    \For{\(i \gets 0\) \KwTo \(n\)}{
        sum \(\gets 0\)\\
        \For{\(j \gets A_{p}[i]\) \KwTo \(A_{p}[i+1]\)}{
            sum \( = \text{sum} + A_{x}[j] \cdot x[A_{j}[j]]\)\\
        }
        \(y[i] \gets\) sum
    }
\end{algorithm}
\medskip


% This algorithm can be parallelized using shared memory parallelization using OpenMPs \texttt{\#pragma omp parallel for} directive in the following manner:
% \medskip

% \begin{algorithm}[H]
%     \caption{Sequential CSR-based SpMV}
%     \SetAlgoVlined
%     \SetKwInOut{Input}{Input}
%     \SetKwInOut{Output}{Output}
%     \Input{\(A_{p},A_{j},A_{x},x\)}
%     \Output{\(y\)\newline}

%     \textbf{\#pragma omp parallel for}\\
%     \For{\(i \gets 0\) \KwTo \(n\)}{
%         \(y[i] \gets 0\)\\
%         \For{\(j \gets A_{p}[i]\) \KwTo \(A_{p}[i+1]\)}{
%             \(y[i] \gets y[i] + A_{x}[j] \cdot x[A_{j}[j]]\)\\
%         }
%     }
% \end{algorithm}

% \section{Computational Density}

\section{Distributed and Shared Memory}
This thesis examines the performance of SpMV in a hybrid environment combining distributed and shared memory. The matrix is divided among multiple nodes in a cluster, where each node has its own local memory. Within each node, OpenMP is used to parallelize the local SpMV computation. After completing local operations, nodes exchange data through the Message Passing Interface (MPI) to assemble the final result. 

\section{Load Balancing}
In a distributed memory system, it is important to partition the matrix so that the computational workload is evenly divided among the processes. This is typically achieved through the use of graph partitioning tools. A widely used tool for this purpose is METIS, which provides the function \texttt{METIS\_PartGraphKway}. Given a parameter \(nprocs\), representing the number of processes the program will run on, \texttt{METIS\_PartGraphKway} attempts to partition the graph into \(nprocs\) equally sized parts. Since finding an optimal partition is an NP-hard problem, METIS does not guarantee an optimal solution, but it produces high-quality approximations that are sufficient for practical use.

% \subsection{Reordering}
% We also want to reorder the matrix because it makes everything easier. W


\subsection{Separator}
When a graph is partitioned into different parts, there will inevitably be some edges which strides across different partitions. The endpoints of these edges are called separators, and will become important when it comes to reducing the communication load of the SpMV computation.

\subsection{Reordering}
We want to reorder the graph in such a way that communication is easy. We do this by checking whether or not a certain entry in \texttt{row\_ptr} is a separator. If it is, we mark it, and later we reorder it such that it is at the beginning of the ranks part of the row pointer. Furthermore we also count the number of separator each rank has. This makes communication real easy.

 
\begin{algorithm}[H]
    \caption{Reordering of Separators}
    \SetAlgoVlined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{\(n_{p}, n_{r}, n_{c}, p, A_{p}, A_{j}, A_{x}\)}
    \Output{Reordered \(A_{p}, A_{j}, A_{x}\)}
    
    newId \(\gets [0] \cdot n_{r}\)\\
    oldId \(\gets [0] \cdot n_{r}\)\\
    id \(\gets 0\)\\
    \(p_{0} \gets 0\)\\
    \phantom{a}\\
    
    \For{\(r \in \{0, \dotsc, n_{p}-1\}\)}{
        \For{\(i \in \{0, \dotsc, n_{r}-1\}\)}{
            \If{\(p[i] = r\)}{
                oldId[id] \(\gets i\)\\
                newId[i] \(\gets\) id\\
                id \(\gets\) id + 1\\
            }
        }
        \(p[r+1] \gets\) id\\
    }
    \phantom{a}\\
    
    newV \(\gets [0] \cdot (n_{r} + 1)\)\\
    newE \(\gets [0] \cdot n_{c}\)\\
    newA \(\gets [0] \cdot n_{c}\)\\
    
    \phantom{a}\\
    \For{\(i \in \{0, \dotsc, n_{r}-1\}\)}{
        degree \(\gets A_{p}[\text{oldId}[i]+1] - A_{p}[\text{oldId}[i]]\)\\
        newV[\(i+1\)] \(\gets\) newV[i] + degree\\
    }
    \phantom{a}\\
    
    \For{\(i \in \{0, \dotsc, n_{r}-1\}\)}{
        degree \(\gets A_{p}[\text{oldId}[i]+1] - A_{p}[\text{oldId}[i]]\)\\
        \For{\(j \in \{0, \dotsc, \text{degree}-1\}\)}{
            newE[newV[i]+j] \(\gets A_{j}[A_{p}[\text{oldId}[i]] + j]\)\\
            newA[newV[i]+j] \(\gets A_{x}[A_{p}[\text{oldId}[i]] + j]\)\\
        }
        \For{\(j \in \{newV[i], \dotsc, newV[i+1]-1\}\)}{
            newE[j] \(\gets\) \text{newId}[newE[j]]\\
        }
    }
    \phantom{a}\\
    
    Overwrite \(A_{p} \gets\) newV, \(A_{j} \gets\) newE, \(A_{x} \gets\) newA\\
\end{algorithm}


\section{Distributed Memory CSR}








