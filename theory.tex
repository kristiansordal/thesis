
\chapter{Theory}

\section{Sparse Matrix-Vector Multiplication}
Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation encountered in many areas of scientific computing. It is especially prominent in solving large systems of linear equations and in large-scale simulations. The matrices involved are typically both very large and very sparse. 

A matrix can in theory be considered sparse if it is worthwhile to treat zero values separately. In theory, this translates to a matrix being less than full, i.e. less than \( \mathcal{O}\left(n^2\right)\) nonzeros for a \(n \times  n\) matrix. However, in the context of sparse linear algebra, sparse means that there is a constant number of nonzeros per row, i.e. \(\mathcal{O}\left(n\right)\) nonzeros per row. The matrices used in scientific computing, such as matrices based on meshes, or graphs such as social networks all have this property. Optimizing the performance of SpMV, particularly through parallel computing techiniques, is crucial for enhancing the efficiency of many scientific applications.

However, SpMV is notoriously difficult to optimize, both in sequential and parallel implementations. One major reason is its inherently low computational intensity.

\section{Definitions}

\begin{definition}[Separator]
    In the context of SpMV, a separator is a node in the graph that has an edge that strides between two partitions.
\end{definition}

\section{Amdahl's Law}

Amdahl’s Law provides a theoretical framework for understanding the limits of performance improvement when additional computational resources are applied to a given problem. It quantifies the potential speedup achieved by optimizing a specific portion of a system, emphasizing that the overall gain is constrained by the proportion of time the optimized component contributes to execution.

\begin{definition}[Amdahl’s Law] The maximum achievable speedup of a computation is limited by the fraction of execution time that remains sequential, even when an arbitrarily large number of parallel resources is employed. \end{definition}

In the context of parallel computing, this principle highlights that while increasing the number of processing units can accelerate the parallelizable portion of a workload, the sequential fraction imposes a fundamental performance ceiling. Formally, if \(S\)  denotes the total speedup, \(t_{p}\) represents the fraction of execution time that can be parallelized, and \(s_{p}\) is the speedup achieved for that parallelizable portion, Amdahl’s Law is expressed as:
\begin{equation} S = \frac{1}{(1 - t_p) + \frac{t_p}{s_p}} \end{equation}
This equation reveals that as \(s_{p} \rightarrow \infty\), the theoretical maximum speedup approaches \(\frac{1}{1-t_{p}}\), illustrating that the non-parallelizable portion becomes the dominant limiting factor in scalability.


\section{Latency}

It is worthwhile to speak on the typical latency numbers for various operations on a computer. 
\input{latencynumber}


\subsection{NUMA Architecture}

Non-Uniform Memory Access (NUMA) refers to a multiprocessor system architecture in which memory access latencies depend on the location of the memory relative to a given processor. Unlike traditional symmetric multiprocessing (SMP) systems, where all processors share equal access times to a centralized memory pool, NUMA architectures consist of multiple processor sockets or nodes, each directly connected to its own local memory. These nodes are interconnected by a high-speed communication network, typically an interconnect, facilitating access to remote memory residing on other nodes.

In NUMA systems, accessing local memory, that is, memory located on the same node as the processor, provides significantly lower latency and higher bandwidth compared to accessing remote memory located on other nodes. Thus, maintaining good memory locality is critical for achieving optimal performance. If data accessed by a processor primarily resides in remote memory, performance degradation may occur due to increased latency and reduced bandwidth, particularly when frequent remote memory accesses occur.

Because of this architecture, software must be carefully designed to maximize locality and minimize remote memory access. Strategies such as the first-touch policy, where memory pages are allocated based on the location of the thread first accessing the memory, are commonly employed to enhance memory locality and overall application performance on NUMA systems.
