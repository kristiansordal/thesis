\chapter{Communication Strategies}

% This thesis evaluates and compares several communication strategies for Sparse Matrix-Vector Multiplication (SpMV) in a parallel, distributed memory setting. During each iteration of SpMV, every process computes a partial result of the output vector 
% \(y\).
% \medskip

% In subsequent iterations, these computed values may be required by other processes to proceed with their own calculations. To ensure correctness, it is therefore necessary to communicate between processes so that each has access to the values it depends on. This section outlines a progression of increasingly efficient strategies for managing this communication.
% \medskip

In parallel implementations of Sparse Matrix-Vector Multiplication (SpMV), effective communication management is critical due to its significant influence on overall performance. Communication often emerges as a bottleneck in distributed-memory systems because the speed at which data moves between nodes is significantly lower than within-node memory access speeds. Consequently, reducing communication volume and optimizing communication patterns can yield substantial performance improvements.
\medskip

This chapter evaluates a series of progressively optimized communication strategies employed in distributed-memory parallel SpMV. Starting from the simplest method, exchanging the entire result vector between all nodes, the strategies become increasingly selective and efficient, focusing specifically on exchanging only the essential data elements required by each node. These approaches leverage knowledge of the matrix structure, partitioning methods, and computational dependencies to minimize communication overhead.
\medskip


% This chapter evaluates a series of progressively optimized communication strategies employed in distributed-memory parallel SpMV. Starting from the simplest method, exchanging the entire result vector between all nodes, the strategies become increasingly selective and efficient, focusing specifically on exchanging only the essential data elements required by each node. These approaches leverage knowledge of the matrix structure, partitioning methods, and computational dependencies to minimize communication overhead.
% \medskip

% The following sections describe each strategy in detail, highlighting their theoretical motivations, implementation specifics using MPI (Message Passing Interface), and potential advantages or disadvantages. The strategies discussed include exchanging the entire vector, exchanging only separator elements, selectively exchanging required separators, and finally, communicating only specific separator values needed by dependent nodes. Additionally, the chapter introduces considerations for fully scalable two-dimensional (2D) communication methods, providing a foundation for scalable performance on large-scale computing clusters.



\begin{figure}[ht]
    \centering
    \incfig{examplegraph}
    \caption{examplegraph}
    \label{fig:examplegraph}
\end{figure}

\section{Strategy A - Exchange entire vector}


The most straightforward approach is to have each rank send all of its computed values of 
\(y\) to every other rank. This ensures that all processes possess a complete and updated copy of the output vector before the next iteration. This strategy can be implemented using MPIâ€™s collective communication operation \texttt{MPI\_Allgatherv}, which accommodates variable message sizes from each rank. Figure \ref{fig:1acomm} illustrates the state of the \(y\) vector before and after communication using this strategy.


% The most basic communication strategy involves sending all values of \(y\) computed by each rank, to every other rank. This communication strategy results in each rank having all newly computed values of \(y\). In order to achieve this, we use MPI's \textit{allgather} fucntion, or more specifically we use \texttt{MPI\_Allgatherv}, which allows for each rank sending a different amount of values to other ranks. Figure \ref{fig:1acomm} illustrates the \(y\) vector before and after communication takes place. 

\begin{figure}[ht]
    \centering
    \incfig{1acomm-2}
    \caption{Contents of each ranks \(x\) vector before and after communication under the \textit{Exchange Entire Vector} communication pattern.}
    \label{fig:1acomm}
\end{figure}


\begin{algorithm}[H]
    \caption{1a - Exchange entire vector}
    \SetAlgoVlined
    % \SetKwInOut{Input}{Input}
    % \SetKwInOut{Output}{Output}
    % \Input{}
    % \Output{\newline}

    \For{each iteration}{
        spmv(g,x,y)\\
        MPI\_Allgatherv(local\_y, sendcount, MPI\_DOUBLE, y, recvcounts, displs, MPI\_DOUBLE, MPI\_COMM\_WORLD)\\
        swap pointers of \(x\) and \(y\)

    }

\end{algorithm}

\section{Strategy B - Exchange only separators}

An improvement to the previous strategy can be achieved by recognizing that only separator values, those required by multiple processes, must be communicated. Non-separator values are used exclusively by the process that computed them and therefore do not need to be communicated.

To facilitate this strategy, separator values are reordered such that they appear at the beginning of each process's local segment of \(y\). Once this structure is established, communication is performed using \texttt{MPI\_Allgatherv}, transmitting only the subset of \(y\) that contains separator values. The number of separators on each process must be known beforehand, which can be computed by counting the number of elements that have neighbours belonging to a different partition.

\begin{figure}[ht]
    \centering
    \incfig{1bcomm}
    \caption{Contents of each ranks \(x\) vector before and after communication under the \textit{Exchange Separators} communication pattern.}
    \label{fig:1bcomm}
\end{figure}


\subsection{Reordering}
After partitioning the matrix into different parts, we obtain a partition vector \(p\), where the \(p[i]\) stores the index of the partition the \(i^{\text{th}}\) entry in \(A_{p}\). It is necessary to reorder the entries in \(A_{p}\) in accordance with the partition vector, such that all entries belonging to the same partition are stored in sequence. The algorithm below gives an outline of how this can be achieved. Here \(n_{p}\) is the number of partitions, \(n_{r}\) is the size of \(A_{p}\), and \(n_{c}\) is the size of \(A_{j}\) and \(A_{x}\).

\begin{algorithm}[H]
    \caption{Reordering of Separators}
    \SetAlgoVlined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{\(n_{p}, n_{r}, n_{c}, p, A_{p}, A_{j}, A_{x}\)}
    \Output{Reordered \(A_{p}, A_{j}, A_{x}\)}
    
    newId \(\gets [0] \cdot n_{r}\)\\
    oldId \(\gets [0] \cdot n_{r}\)\\
    id \(\gets 0\)\\
    \(p_{0} \gets 0\)\\
    \phantom{a}\\
    
    \For{\(r \in \{0, \dotsc, n_{p}-1\}\)}{
        \For{\(i \in \{0, \dotsc, n_{r}-1\}\)}{
            \If{\(p[i] = r\)}{
                oldId[id] \(\gets i\)\\
                newId[i] \(\gets\) id\\
                id \(\gets\) id + 1\\
            }
        }
        \(p[r+1] \gets\) id\\
    }
    \phantom{a}\\
    
    newV \(\gets [0] \cdot (n_{r} + 1)\)\\
    newE \(\gets [0] \cdot n_{c}\)\\
    newA \(\gets [0] \cdot n_{c}\)\\
    
    \phantom{a}\\
    \For{\(i \in \{0, \dotsc, n_{r}-1\}\)}{
        degree \(\gets A_{p}[\text{oldId}[i]+1] - A_{p}[\text{oldId}[i]]\)\\
        newV[\(i+1\)] \(\gets\) newV[i] + degree\\
    }
    \phantom{a}\\
    
    \For{\(i \in \{0, \dotsc, n_{r}-1\}\)}{
        degree \(\gets A_{p}[\text{oldId}[i]+1] - A_{p}[\text{oldId}[i]]\)\\
        \For{\(j \in \{0, \dotsc, \text{degree}-1\}\)}{
            newE[newV[i]+j] \(\gets A_{j}[A_{p}[\text{oldId}[i]] + j]\)\\
            newA[newV[i]+j] \(\gets A_{x}[A_{p}[\text{oldId}[i]] + j]\)\\
        }
        \For{\(j \in \{newV[i], \dotsc, newV[i+1]-1\}\)}{
            newE[j] \(\gets\) \text{newId}[newE[j]]\\
        }
    }
    \phantom{a}\\
    
    Overwrite \(A_{p} \gets\) newV, \(A_{j} \gets\) newE, \(A_{x} \gets\) newA\\
\end{algorithm}
% It becomes evident that this is a better strategy when we realize that it is not necessary to communicate non-separator values of \(y\), as these values are only ever needed by the rank they are computed on. Separators on the other hand, are needed by other ranks, and therefore need to be communicated. This strategy can be implemented by reordering and counting the number of separators on each rank, and only sending these values to all other ranks, again using \textit{allgather}.


% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{1acomm.png}
%         \caption{Caption for image 1}
%         \label{fig:1bcomm1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{1bcommdone.png}
%         \caption{Caption for image 2}
%         \label{fig:1bcomm2}
%     \end{subfigure}
%     \caption{Main figure caption describing both subfigures}
%     \label{fig:1bcomm}
% \end{figure}


\section{Strategy C - Exchange only required separators}
Further reduciton to the communication volume can be achieved by observing that not all separator values are required by every process. As the number of partitions increases, the set of dependencies between partitions tends towards sparcity. As a consequence of this, certain sets of separators may only need to be communicated to a given subset of processes. Using this strategy, each process only communicates its set of separator values to the processes that require them. The communication pattern in illustrated in Figure \ref{fig:1ccomm}, and the \autoref{alg:1ccomm} shows how this can be implemented.
% Another observation that can be made in order to further reduce the communication load comes when we realize that not every rank necessarily needs every separator. As we increase the number of partitions in the matrix, we increase the chance that there exists ranks whose separators are disjoint. If this is the case, we can further reduce the communication load by only sending a ranks separator to another rank if it is necessary 


% Not all ranks need all separators. If rank 0 has some number of separators, none of which are needed by say rank 2, we don't need to communicate them. This strategy further reduces the communication load.

\begin{algorithm}[H]
    \label{alg:1ccomm}
    \caption{Exchange only required separators}
    \SetAlgoVlined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{y, rank,size, displacements, sendItems, sendCount}
    \Output{\newline}

    % mpiRequests \(\gets\) [MPI\_Request] \(\cdot\)  \(2 \cdot \) size\\
    % reqCount \(\gets\) 0\\
    MPI\_Requests \(\gets\) 0\\
 
    \For{r = 0; r < size; r++}{
        \If{rank = r or sendItems[r][rank] = 0}{
            continue\\
        }

        Post non-blocking MPI receive for \(sendCount[r]\) elements at address \(y + displacements[r]\)\\
        MPI\_Requests++\\
    }

    \For{r = 0; r < size; r++}{
    \If{rank = r or sendItems[r][rank] = 0}{
        continue\\
    }
    Post non-blocking MPI send of \(sendCount[rank]\) elements to address \(y + displacements[rank]\)\\
    MPI\_Requests++\\
    }
    Wait for all non-blocking requests to complete\\
\end{algorithm}


\begin{figure}[ht]
    \centering
    \incfig{1ccomm}
    \caption{Contents of each ranks \(x\) vector before and after communication under the \textit{Exchange Reuired Separators} communication pattern.}
    \label{fig:1ccomm}
\end{figure}

\section{Exchange only required separator values}
The final strategy aims to minimize communication overhead by transmitting only the exact subset of separator values that are both computed by and required for inter-process computation. If a specific separator value computed by one process is needed by exactly one other process, then only that single recipient receives the value.

This approach eliminates all unnecessary data transfers but introduces additional complexity in managing communication schedules. Dependencies must be mapped at a fine-grained level, and communication patterns must be explicitly tailored to the structure of the matrix and its partitioning.



\begin{figure}[ht]
    \centering
    \incfig{1dcomm}
    \caption{Contents of each ranks \(x\) vector before and after communication under the \textit{Exchange Reuired Elements} communication pattern.}
    \label{fig:1dcomm}
\end{figure}


\section{Memory Scalable Strategies}
The communication strategies discussed so far all have a common problem that prevents them from scaling to large matrices. These strategies all store the entire vector \(x\), and will run into performance issues when \(x\) is so large that it doesn't fit into memory. Usually, this is not a problem when SpMV is ran on CPUs, as they have large amounts of memory. Even on GPUs this problem might not be encountered, as modern GPUs have sufficient memory for large matrices.
\medskip

Instead of storing the entire vector, each rank only stores its local part of the vector. In addition, it is necessary to allocate enough space for the separators elements that are needed from the other ranks. In order to achieve this, \(x\) is renumbered such that every ranks part of the vector is 0-indexed. For the local part of the \(x\) vector this is done by simply subtracting the position of the first local entry assigned to that rank from each entry in the local vector. It is slightly more complicated to renumber the separator elements, and the technicalities of how to do this will not be discussed here.
\medskip

\begin{algorithm}[H]
    \label{alg:req_sep_exchange}
    \caption{Exchange Required Separators}
    \SetAlgoVlined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{c, $V_n$, rank, size}
    \Output{Updated vector $V_n$}

    totalSend, totalRecv $\gets$ 0\\
    \For{i = 0; i < size; i++}{
        totalSend $\gets$ totalSend + c.sendCount[i]\\
        totalRecv $\gets$ totalRecv + c.receiveCount[i]\\
    }

    Allocate sendBuffer[totalSend], recvBuffer[totalRecv]\\

    sendOffset $\gets$ 0\\
    \For{i = 0; i < size; i++}{
        \For{j = 0; j < c.sendCount[i]; j++}{
            sendBuffer[sendOffset++] $\gets$ $V_n$[c.sendItems[i][j]]\\
        }
    }

    Compute sendDispls, recvDispls as prefix sums of c.sendCount, c.receiveCount\\

    MPI\_Ialltoallv(sendBuffer, c.sendCount, sendDispls, recvBuffer, c.receiveCount, recvDispls)\\

    MPI\_Waitall()\\

    recvOffset $\gets$ 0\\
    \For{i = 0; i < size; i++}{
        \For{j = 0; j < c.receiveCount[i]; j++}{
            $V_n$[c.receiveItems[i][j]] $\gets$ recvBuffer[recvOffset++]\\
        }
    }

    Free sendBuffer, recvBuffer, sendDispls, recvDispls\\
\end{algorithm}

% There is no specific communication pattern that are required for 

\begin{figure}[ht]
    \centering
    \incfig{2dcomm}
    \caption{Global indexing vs. Local indexing of \(x\).}
    \label{fig:2dcomm}
\end{figure}








